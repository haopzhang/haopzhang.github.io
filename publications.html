<!DOCTYPE html>
<html lang="en" class="ace ace-card-on ace-tab-nav-on ace-main-nav-on ace-sidebar-on" data-theme-color="#c0e3e7">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Haopeng Zhang's Webpage</title>
    <meta name="description" content="">

    <link rel="apple-touch-icon" href="apple-touch-icon.png">
    <link rel="shortcut icon" href="favicon.ico">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Quicksand:400,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Pacifico" rel="stylesheet">

    <!-- Icon Fonts -->
    <link href="fonts/icomoon/style.css" rel="stylesheet">

    <!-- Styles -->
    <link href="js/plugins/highlight/solarized-light.css" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <!-- Modernizer -->
    <script type="text/javascript" src="js/vendor/modernizr-3.3.1.min.js"></script>
  </head>
  <body>
     <div class="ace-wrapper">
         <header id="ace-header" class="ace-container-shift ace-logo-in ace-head-boxed ace-nav-right">
             <div class="ace-head-inner">
                 <div class="ace-head-container ace-container">
                     <div class="ace-head-row">
                         <div id="ace-head-col2" class="ace-head-col text-right">
                             <div class="ace-nav-container ace-container hidden-sm hidden-xs">
                                 <nav id="ace-main-nav">
                                    <ul class="clear-list">
									   <li><a href="index.html">About</a></li>
                                       <li><a href="research.html">Research</a>
                                       <li><a href="publications.html">Publications</a></li>
                                       <li><a href="students.html">Students</a></li>
                                       <li><a href="teaching.html">Teaching</a></li>
                                       <li><a href="joinus.html">Join Us</a></li>
									</ul>
                                 </nav>
                             </div>
                         </div>
                         
                     </div>
                 </div><!-- .ace-container -->
             </div><!-- .ace-head-inner -->
        </header><!-- #ace-header -->

        
        <nav id="ace-nav-sm" class="ace-nav hidden-lg hidden-md">
            <ul class="clear-list">
				<li>
					<a href="index.html" data-tooltip="Home"><img class="avatar avatar-42" src="img/uploads/avatar/avatar-42x42.png" alt=""></a>
				</li>
				<li>
					<a href="research.html" data-tooltip="Research"><span class="ace-icon ace-icon-experience"></span></a>
				</li>
                <li>
					<a href="publications.html" data-tooltip="Publications"><span class="ace-icon ace-icon-references"></span></a>
				</li>
                <li>
					<a href="students.html" data-tooltip="Students"><span class="ace-icon ace-icon-portfolio"></span></a>
				</li>
				<li>
					<a href="teaching.html" data-tooltip="Teaching"><span class="ace-icon ace-icon-blog"></span></a>
				</li>
				<li>
					<a href="joinus.html" data-tooltip="Join us"><span class="ace-icon ace-icon-contact"></span></a>
				</li>
			</ul>
        </nav><!-- #ace-tab-nav-sm -->

        <article id="ace-card" class="ace-card bg-primary">
			<div class="ace-card-inner text-center">
				<img class="avatar avatar-195" src="img/uploads/avatar/avatar-195x195.png" width="195" height="195" alt="">
				<h2>Haopeng Zhang</h2>
				<p class="text-muted">Assistant Professor | Beihang University</p>
			</div>
		</article><!-- #ace-card -->

        <div id="ace-content" class="ace-container-shift">
            <div class="ace-container">

                
                    <div id="ace-nav-wrap" class="hidden-sm hidden-xs">
                        <div class="ace-nav-cont">
                            <div id="ace-nav-scroll">
                                <nav id="ace-nav" class="ace-nav">
                                    <ul class="clear-list">
										<li>
											<a href="index.html" data-tooltip="Home"><img class="avatar avatar-42" src="img/uploads/avatar/avatar-42x42.png" alt=""></a>
										</li>									
										<li>
											<a href="research.html" data-tooltip="Research"><span class="ace-icon ace-icon-blog"></span></a>
										</li>
										<li>
											<a href="publications.html" data-tooltip="Publications"><span class="ace-icon ace-icon-references"></span></a>
										</li>
										<li>
											<a href="students.html" data-tooltip="Students"><span class="ace-icon ace-icon-portfolio"></span></a>
										</li>
										<li>
											<a href="teaching.html" data-tooltip="Teaching"><span class="ace-icon ace-icon-experience"></span></a>
										</li>
										<li>
											<a href="joinus.html" data-tooltip="Join us"><span class="ace-icon ace-icon-contact"></span></a>
										</li>
									</ul>
                                </nav>
                            </div>

                            <div id="ace-nav-tools" class="hidden">
                                <span class="ace-icon ace-icon-dots-three-horizontal"></span>

                                <button id="ace-nav-arrow" class="clear-btn">
                                    <span class="ace-icon ace-icon-chevron-thin-down"></span>
                                </button>
                            </div>
                        </div>
                        <div class="ace-nav-btm"></div>
                    </div><!-- .ace-nav-wrap -->

                <div class="ace-paper-stock">
                    <main class="ace-paper clearfix">
                        <div class="ace-paper-cont clear-mrg">
						
						<!-- START: PAGE CONTENT -->

    <div class="padd-box">

        <section class="section clear-mrg">
            <h2 class="title-lg text-upper">Publications</h2>

            <h3>2019</h3>
            <table><tbody>

                <tr> <!-- An Paper Sensors-->
                  <td width="25%" valign="top"><p>
                    <img src="img/ga/ZhangSensors2019.png" alt="ZhangSensors2019" width="200">
                  </p></td>
                  <td width="75%" valign="top">
                    <p>
                      <b>A Comparable Study of CNN-Based Single Image Super-Resolution for Space-Based Imaging Sensors</b>
                      <p><b>Haopeng Zhang*</b>, Pengrui Wang, Cong Zhang, and Zhiguo Jiang</p>
                      <i>Sensors, 2019, 19(14): 3234.</i>
                      <br>          
                    </p>
                    <a href="https://www.mdpi.com/1424-8220/19/14/3234/pdf" target="_blank" textvalue="https://www.mdpi.com/1424-8220/19/14/3234/pdf" class="pf-btn-view btn btn-primary">PDF</a>
                    <a href="#pf-popup-ab-ZhangSensors2019" class="pf-btn-view btn btn-primary">Abstract</a>
                    <a href="#pf-popup-bib-ZhangSensors2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                    
                    <div class="pf-grid-item photography">
                      <div id="pf-popup-ab-ZhangSensors2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">Abstract</h2>
                                  <p>In the case of space-based space surveillance (SBSS), images of the target space objects captured by space-based imaging sensors usually suffer from low spatial resolution due to the extremely long distance between the target and the imaging sensor. Image super-resolution is an effective data processing operation to get informative high resolution images. In this paper, we comparably study four recent popular models for single image super-resolution based on convolutional neural networks (CNNs) with the purpose of space applications. We specially fine-tune the super-resolution models designed for natural images using simulated images of space objects, and test the performance of different CNN-based models in different conditions that are mainly considered for SBSS. Experimental results show the advantages and drawbacks of these models, which could be helpful for the choice of proper CNN-based super-resolution method to deal with image data of space objects.
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                    <div class="pf-grid-item photography">
                        <div id="pf-popup-bib-ZhangSensors2019" class="pf-popup clearfix">
                                <div class="pf-popup-info clear-mrg">
                                    <h2 class="text-upper">BibTeX</h2>
                                    <p>
                                        @Article{ZhangSensors2019,<br> 
                                          AUTHOR = {Haopeng Zhang and Pengrui Wang and Cong Zhang and Zhiguo Jiang},<br> 
                                          TITLE = {A Comparable Study of CNN-Based Single Image Super-Resolution for Space-Based Imaging Sensors},<br> 
                                          JOURNAL = {Sensors},<br> 
                                          VOLUME = {19},<br> 
                                          YEAR = {2019},<br> 
                                          NUMBER = {14},<br> 
                                          ARTICLE-NUMBER = {3234},<br> 
                                          ISSN = {1424-8220},<br> 
                                          DOI = {10.3390/s19143234}<br> 
                                          } 
                                    </p>
                                </div><!-- .pf-popup-info -->
                        </div><!-- .pf-popup -->
                      </div><!-- .pf-grid-item -->
                  </td>
                </tr> <!-- Paper End Here -->

                <tr> <!-- An Paper IVC-->
                  <td width="25%" valign="top"><p>
                    <img src="img/ga/ZhangIVC2019.png" alt="ZhangIVC2019" width="200">
                  </p></td>
                  <td width="75%" valign="top">
                    <p>
                      <b>Real-Time 6D Pose Estimation from a Single RGB Image</b>
                      <p>Xin Zhang, Zhiguo Jiang, and <b>Haopeng Zhang*</b></p>
                      <i>Image and Vision Computing, 2019, 89: 1-11.</i>
                      <br>          
                    </p>
                    <a href="preprint/ZhangIVC2019preprint.pdf" target="_blank" textvalue="preprint/ZhangIVC2019preprint.pdf" class="pf-btn-view btn btn-primary">Preprint</a>
                    <a href="https://doi.org/10.1016/j.imavis.2019.06.013" target="_blank" textvalue="https://doi.org/10.1016/j.imavis.2019.06.013" class="pf-btn-view btn btn-primary">PDF link</a>
                    <a href="#pf-popup-ab-ZhangIVC2019" class="pf-btn-view btn btn-primary">Abstract</a>
                    <a href="#pf-popup-bib-ZhangIVC2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                    
                    <div class="pf-grid-item photography">
                      <div id="pf-popup-ab-ZhangIVC2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">Abstract</h2>
                                  <p>We propose an end-to-end deep learning architecture for simultaneously detecting objects and recovering 6D poses in an RGB image. Concretely, we extend the 2D detection pipeline with a pose estimation module to indirectly regress the image coordinates of the object’s 3D vertices based on 2D detection results. Then the object’s 6D pose can be estimated using a Perspective-n-Point algorithm without any post-refinements. Moreover, we elaborately design a backbone structure to maintain spatial resolution of low level features for pose estimation task. Compared with state-of-the-art RGB based pose estimation methods, our approach achieves competitive or superior performance on two benchmark datasets at an inference speed of 25 fps on a GTX 1080Ti GPU, which is capable of real-time processing. 
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                    <div class="pf-grid-item photography">
                        <div id="pf-popup-bib-ZhangIVC2019" class="pf-popup clearfix">
                                <div class="pf-popup-info clear-mrg">
                                    <h2 class="text-upper">BibTeX</h2>
                                    <p>
                                        @Article{ZhangIVC2019,<br> 
                                          AUTHOR = {Xin Zhang and Zhiguo Jiang and Haopeng Zhang},<br> 
                                          TITLE = {Real-Time 6D Pose Estimation from a Single RGB Image},<br> 
                                          JOURNAL = {Image and Vision Computing},<br> 
                                          VOLUME = {89},<br> 
                                          YEAR = {2019},<br> 
                                          NUMBER = {},<br> 
                                          PAGES = {1-11},<br> 
                                          ISSN = {0262-8856},<br> 
                                          DOI = {10.1016/j.imavis.2019.06.013}<br> 
                                          } 
                                    </p>
                                </div><!-- .pf-popup-info -->
                        </div><!-- .pf-popup -->
                      </div><!-- .pf-grid-item -->
                  </td>
                </tr> <!-- Paper End Here -->

                <tr> <!-- An Paper IJAE-->
                  <td width="25%" valign="top"><p>
                    <img src="img/ga/ZhangIJAE2019.png" alt="ZhangIJAE2019" width="200">
                  </p></td>
                  <td width="75%" valign="top">
                    <p>
                      <b>Vision-Based Satellite Recognition and Pose Estimation Using Gaussian Process Regression</b>
                      <p><b>Haopeng Zhang*</b>, Cong Zhang, Zhiguo Jiang, Yuan Yao, and Gang Meng</p>
                      <i>International Journal of Aerospace Engineering, 2019, 2019: 5921246.</i>
                      <br>          
                    </p>
                    <a href="http://downloads.hindawi.com/journals/ijae/2019/5921246.pdf" target="_blank" textvalue="http://downloads.hindawi.com/journals/ijae/2019/5921246.pdf" class="pf-btn-view btn btn-primary">PDF</a>
                    <a href="#pf-popup-ab-ZhangIJAE2019" class="pf-btn-view btn btn-primary">Abstract</a>
                    <a href="#pf-popup-bib-ZhangIJAE2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                    <div class="pf-grid-item photography">
                      <div id="pf-popup-ab-ZhangIJAE2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">Abstract</h2>
                                  <p>In this paper, we address the problem of vision-based satellite recognition and pose estimation, which is to recognize the satellite from multiviews and estimate the relative poses using imaging sensors. We propose a vision-based method to solve these two problems using Gaussian process regression (GPR). Assuming that the regression function mapping from the image (or feature) of the target satellite to its category or pose follows a Gaussian process (GP) properly parameterized by a mean function and a covariance function, the predictive equations can be easily obtained by a maximum-likelihood approach when training data are given. These explicit formulations can not only offer the category or estimated pose by the mean value of the predicted output but also give its uncertainty by the variance which makes the predicted result convincing and applicable in practice. Besides, we also introduce a manifold constraint to the output of the GPR model to improve its performance for satellite pose estimation. Extensive experiments are performed on two simulated image datasets containing satellite images of 1D and 2D pose variations, as well as different noises and lighting conditions. Experimental results validate the effectiveness and robustness of our approach.
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                    <div class="pf-grid-item photography">
                        <div id="pf-popup-bib-ZhangIJAE2019" class="pf-popup clearfix">
                                <div class="pf-popup-info clear-mrg">
                                    <h2 class="text-upper">BibTeX</h2>
                                    <p>
                                        @Article{ZhangIJAE2019,<br> 
                                          AUTHOR = {Haopeng Zhang and Cong Zhang and  Zhiguo Jiang and  Yuan Yao and  and Gang Meng},<br> 
                                          TITLE = {Vision-Based Satellite Recognition and Pose Estimation Using Gaussian Process Regression},<br> 
                                          JOURNAL = {International Journal of Aerospace Engineering},<br> 
                                          VOLUME = {2019},<br> 
                                          YEAR = {2019},<br> 
                                          ARTICLE-NUMBER = {5921246},<br> 
                                          ISSN = {1687-5966},<br> 
                                          DOI = {10.1155/2019/5921246}<br> 
                                          } 
                                    </p>
                                </div><!-- .pf-popup-info -->
                        </div><!-- .pf-popup -->
                      </div><!-- .pf-grid-item -->
                  </td>
                </tr> <!-- Paper End Here -->

                <tr> <!-- An Paper RS-->
                  <td width="25%" valign="top"><p>
                    <img src="img/ga/ZhangRS2019.jpg" alt="ZhangRS2019" width="200">
                  </p></td>
                  <td width="75%" valign="top">
                    <p>
                      <b>Deep Learning Based Fossil-Fuel Power Plant Monitoring in High Resolution Remote Sensing Images: A Comparative Study</b>
                      <p><b>Haopeng Zhang*</b> and Qin Deng</p>
                      <i>Remote Sensing, 2019, 11(9): 1117.</i>
                      <br>          
                    </p>
                    <a href="https://www.mdpi.com/2072-4292/11/9/1117/pdf" target="_blank" textvalue="https://www.mdpi.com/2072-4292/11/9/1117/pdf" class="pf-btn-view btn btn-primary">PDF</a>
                    <a href="#pf-popup-ab-ZhangRS2019" class="pf-btn-view btn btn-primary">Abstract</a>
                    <a href="#pf-popup-bib-ZhangRS2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                    <a href="https://github.com/SPDQ/Power-Plant-Detection-in-RSI" target="_blank" textvalue="https://github.com/SPDQ/Power-Plant-Detection-in-RSI" class="pf-btn-view btn btn-primary">Code</a>
                    <div class="pf-grid-item photography">
                      <div id="pf-popup-ab-ZhangRS2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">Abstract</h2>
                                  <p>The frequent hazy weather with air pollution in North China has aroused wide attention in the past few years. 
                                    One of the most important pollution resource is the anthropogenic emission by fossil-fuel power plants. 
                                    To relieve the pollution and assist urban environment monitoring, it is necessary to continuously monitor the working status of power plants. 
                                    Satellite or airborne remote sensing provides high quality data for such tasks. 
                                    In this paper, we design a power plant monitoring framework based on deep learning to automatically detect the power plants 
                                    and determine their working status in high resolution remote sensing images (RSIs). 
                                    To this end, we collected a dataset named BUAA-FFPP60 containing RSIs of over 60 fossil-fuel power plants in the Beijing-Tianjin-Hebei region 
                                    in North China, which covers about 123 km2 of an urban area. We compared eight state-of-the-art deep learning models and 
                                    comprehensively analyzed their performance on accuracy, speed, and hardware cost. 
                                    Experimental results illustrate that our deep learning based framework can effectively detect the fossil-fuel power 
                                    plants and determine their working status with mean average precision up to 0.8273, 
                                    showing good potential for urban environment monitoring. 
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                    <div class="pf-grid-item photography">
                        <div id="pf-popup-bib-ZhangRS2019" class="pf-popup clearfix">
                                <div class="pf-popup-info clear-mrg">
                                    <h2 class="text-upper">BibTeX</h2>
                                    <p>
                                        @Article{ZhangRS2019,<br> 
                                          AUTHOR = {Haopeng Zhang and Qin Deng},<br> 
                                          TITLE = {Deep Learning Based Fossil-Fuel Power Plant Monitoring in High Resolution Remote Sensing Images: A Comparative Study},<br> 
                                          JOURNAL = {Remote Sensing},<br> 
                                          VOLUME = {11},<br> 
                                          YEAR = {2019},<br> 
                                          NUMBER = {9},<br> 
                                          ARTICLE-NUMBER = {1117},<br> 
                                          ISSN = {2072-4292},<br> 
                                          DOI = {10.3390/rs11091117}<br> 
                                          } 
                                    </p>
                                </div><!-- .pf-popup-info -->
                        </div><!-- .pf-popup -->
                      </div><!-- .pf-grid-item -->
                  </td>
                </tr> <!-- Paper End Here -->


              <tr> <!-- An Paper RS-->
                <td width="25%" valign="top"><p>
                  <img src="img/ga/YaoRS2019.png" alt="YaoRS2019" width="200">
                </p></td>
                <td width="75%" valign="top">
                  <p>
                    <b>On-Board Ship Detection in Micro-Nano Satellite Based on Deep Learning and COTS Component</b>
                    <p>Yuan Yao, Zhiguo Jiang, <b>Haopeng Zhang*</b>, and Yu Zhou</p>
                    <i>Remote Sensing, 2019, 11(7): 762.</i>
                    <br>          
                  </p>
                  <a href="https://www.mdpi.com/2072-4292/11/7/762/pdf" target="_blank" textvalue="https://www.mdpi.com/2072-4292/11/7/762/pdf" class="pf-btn-view btn btn-primary">PDF</a>
                  <a href="#pf-popup-ab-YaoRS2019" class="pf-btn-view btn btn-primary">Abstract</a>
                  <a href="#pf-popup-bib-YaoRS2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                  <div class="pf-grid-item photography">
                    <div id="pf-popup-ab-YaoRS2019" class="pf-popup clearfix">
                            <div class="pf-popup-info clear-mrg">
                                <h2 class="text-upper">Abstract</h2>
                                <p>Micro-nano satellites have provided a large amount of remote sensing images for many earth observation applications. However, the hysteresis of satellite-ground mutual communication of massive remote sensing images and the low efficiency of traditional information processing flow have become the bottlenecks for the further development of micro-nano satellites. To solve this problem, this paper proposes an on-board ship detection scheme based on deep learning and Commercial Off-The-Shelf (COTS) component, which can be used to achieve near real-time on-board processing by micro-nano satellite computing platform. The on-board ship detection algorithm based on deep learning consists of a feature extraction network, Region Proposal Network (RPN) with square anchors, Global Average Pooling (GAP), and Bigger-Left Non-Maximum Suppression (BL-NMS). With the help of high performance COTS components, the proposed scheme can extract target patches and valuable information from remote sensing images quickly and accurately. A ground demonstration and verification system is built to verify the feasibility and effectiveness of our scheme. Our method achieves the performance with 95.9% recall and 80.5% precision in our dataset. Experimental results show that the scheme has a good application prospect in micro-nano satellites with limited power and computing resources.
                                  </p>
                            </div><!-- .pf-popup-info -->
                    </div><!-- .pf-popup -->
                  </div><!-- .pf-grid-item -->
                  <div class="pf-grid-item photography">
                      <div id="pf-popup-bib-YaoRS2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">BibTeX</h2>
                                  <p>
                                      @Article{YaoRS2019,<br> 
                                        AUTHOR = {Yao, Yuan and Jiang, Zhiguo and Zhang, Haopeng and Zhou, Yu},<br> 
                                        TITLE = {On-Board Ship Detection in Micro-Nano Satellite Based on Deep Learning and COTS Component},<br> 
                                        JOURNAL = {Remote Sensing},<br> 
                                        VOLUME = {11},<br> 
                                        YEAR = {2019},<br> 
                                        NUMBER = {7},<br> 
                                        ARTICLE-NUMBER = {762},<br> 
                                        ISSN = {2072-4292},<br> 
                                        DOI = {10.3390/rs11070762}<br> 
                                        } 
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                </td>
              </tr> <!-- Paper End Here -->

              <tr> <!-- An Paper Access-->
                <td width="25%" valign="top"><p>
                  <img src="img/ga/ZhangAccess2019.jpg" alt="ZhangAccess2019" width="200">
                </p></td>
                <td width="75%" valign="top">
                  <p>
                    <b>Star Detection and Accurate Centroiding for the Geosynchronous Interferometric Infrared Sounder of Fengyun-4A</b>
                    <p><b>Haopeng Zhang</b>, Yi Su, Lei Yang, Jian Shang, Chengbao Liu, Jing Wang, Shengxiong Zhou, Zhiguo Jiang, and Zhiqing Zhang</p>
                    <i>IEEE Access, 2019, 7: 18510-18520.</i>
                    <br>          
                  </p>
                  <a href="https://ieeexplore.ieee.org/iel7/6287639/6514899/08629974.pdf" target="_blank" textvalue="https://ieeexplore.ieee.org/iel7/6287639/6514899/08629974.pdf" class="pf-btn-view btn btn-primary">PDF</a>
                  <a href="#pf-popup-ab-ZhangAccess2019" class="pf-btn-view btn btn-primary">Abstract</a>
                  <a href="#pf-popup-bib-ZhangAccess2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                  <div class="pf-grid-item photography">
                    <div id="pf-popup-ab-ZhangAccess2019" class="pf-popup clearfix">
                            <div class="pf-popup-info clear-mrg">
                                <h2 class="text-upper">Abstract</h2>
                                <p>Extracting accurate star centroids in the observed star images is one of the key problems for image navigation of the geosynchronous interferometric infrared sounder (GIIRS) of Fengyun-4A Satellite (FY-4A), the first scientific experimental satellite of the new generation of Chinese geostationary meteorological satellite Fengyun-4 series. Compared with star sensors which are widely used for star observation, it is challenging to detect the 2×2 star spot from the focused star images of GIIRS and calculate the star centroid in high precision. In this paper, we propose a star detection and centroiding method based on trajectory search and trajectory fitting. Since the launch of FY-4A in December 2016, our centroiding method has been tested in-orbit for over two years. The extensive experiments show that the star centroiding error of our method is less than 0.3 pixels, which makes an important contribution to image navigation of FY-4A.
                                </p>
                            </div><!-- .pf-popup-info -->
                    </div><!-- .pf-popup -->
                  </div><!-- .pf-grid-item -->
                  <div class="pf-grid-item photography">
                      <div id="pf-popup-bib-ZhangAccess2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">BibTeX</h2>
                                  <p>
                                      @Article{ZhangAccess2019,<br> 
                                        author={H. {Zhang} and Y. {Su} and L. {Yang} and J. {Shang} and C. {Liu} and J. {Wang} and S. {Zhou} and Z. {Jiang} and Z. {Zhang}},<br> 
                                        journal={IEEE Access},<br> 
                                        title={Star Detection and Accurate Centroiding for the Geosynchronous Interferometric Infrared Sounder of Fengyun-4A},<br> 
                                        year={2019},<br> 
                                        volume={7},<br> 
                                        number={},<br> 
                                        pages={18510-18520},<br> 
                                        doi={10.1109/ACCESS.2019.2896148},<br>
                                        ISSN={2169-3536}<br> 
                                        } 
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                </td>
              </tr> <!-- Paper End Here -->

              <tr> <!-- An Paper CMPB-->
                <td width="25%" valign="top"><p>
                  <img src="img/ga/ZhengCMPB2019.jpg" alt="ZhengCMPB2019" width="200">
                </p></td>
                <td width="75%" valign="top">
                  <p>
                    <b>Adaptive Color Deconvolution For Histological WSI Normalization</b>
                    <p>Yushan Zheng, Zhiguo Jiang, <b>Haopeng Zhang*</b>, Fengying Xie, Jun Shi, and Chenghai Xue</p>
                    <i>Computer Methods and Programs in Biomedicine, 2019, 170: 107-120.</i>
                    <br>          
                  </p>
                  <a href=""  class="pf-btn-view btn btn-primary">PDF</a>
                  <a href="#pf-popup-ab-ZhengCMPB2019" class="pf-btn-view btn btn-primary">Abstract</a>
                  <a href="#pf-popup-bib-ZhengCMPB2019" class="pf-btn-view btn btn-primary">BibTeX</a>
                  <div class="pf-grid-item photography">
                    <div id="pf-popup-ab-ZhengCMPB2019" class="pf-popup clearfix">
                            <div class="pf-popup-info clear-mrg">
                                <h2 class="text-upper">Abstract</h2>
                                <p><b>Background and Objective</b></br>
                                 Color consistency of histological images is significant for developing reliable computer-aided diagnosis (CAD) systems. However, the color appearance of digital histological images varies across different specimen preparations, staining, and scanning situations. This variability affects the diagnosis and decreases the accuracy of CAD approaches. It is important and challenging to develop effective color normalization methods for digital histological images.
                                </br><b>Methods</b></br>
                                We proposed a novel adaptive color deconvolution (ACD) algorithm for stain separation and color normalization of hematoxylin-eosin-stained whole slide images (WSIs). To avoid artifacts and reduce the failure rate of normalization, multiple prior knowledges of staining are considered and embedded in the ACD model. To improve the capacity of color normalization for various WSIs, an integrated optimization is designed to simultaneously estimate the parameters of the stain separation and color normalization. The solving of ACD model and application of the proposed method involves only pixel-wise operation, which makes it very efficient and applicable to WSIs.
                                </br><b>Results</b></br>
                                The proposed method was evaluated on four WSI-datasets including breast, lung and cervix cancers and was compared with 6 state-of-the-art methods. The proposed method achieved the most consistent performance in color normalization according to the quantitative metrics. Through a qualitative assessment for 500 WSIs, the failure rate of normalization was 0.4% and the structure and color artifacts were effectively avoided. Applied to CAD methods, the area under receiver operating characteristic curve for cancer image classification was improved from 0.842 to 0.914. The average time of solving the ACD model is 2.97 s.
                                </br><b>Conclusions</b></br>
                                The proposed ACD model has prone effective for color normalization of hematoxylin-eosin-stained WSIs in various color appearances. The model is robust and can be applied to WSIs containing different lesions. The proposed model can be efficiently solved and is effective to improve the performance of cancer image recognition, which is adequate for developing automatic CAD programs and systems based on WSIs.</p>
                                </p>
                            </div><!-- .pf-popup-info -->
                    </div><!-- .pf-popup -->
                  </div><!-- .pf-grid-item -->
                  <div class="pf-grid-item photography">
                      <div id="pf-popup-bib-ZhengCMPB2019" class="pf-popup clearfix">
                              <div class="pf-popup-info clear-mrg">
                                  <h2 class="text-upper">BibTeX</h2>
                                  <p>
                                      @article{ZhengCMPB2019,<br> 
                                        title   = {Adaptive color deconvolution for histological WSI normalization},<br> 
                                        author  = {Yushan Zheng and Zhiguo Jiang and Haopeng Zhang and Fengying Xie and Jun Shi and Chenghai Xue},<br> 
                                        journal = {Computer Methods and Programs in Biomedicine},<br> 
                                        volume  = {170},<br> 
                                        pages   = {107-120},<br> 
                                        doi     = {10.1016/j.cmpb.2019.01.008},<br> 
                                        year    = {2019}<br> 
                                      }
                                  </p>
                              </div><!-- .pf-popup-info -->
                      </div><!-- .pf-popup -->
                    </div><!-- .pf-grid-item -->
                </td>
              </tr> <!-- Paper End Here -->

            </tbody></table>

            <h3>2018</h3>
            <table><tbody>
                <tr> <!-- An Paper TAES-->
                        <td width="25%" valign="top"><p>
                          <img src="img/ga/ZhangTAES2018.png" alt="ZhangTAES2018" width="200">
                        </p></td>
                        <td width="75%" valign="top">
                          <p>
                            <b>Vision-based Pose Estimation for Textureless Space Objects by Contour Points Matching</b>
                            <p>Xin Zhang, Zhiguo Jiang, <b>Haopeng Zhang*</b>, and Quanmiao Wei</p>
                            <i>IEEE Transactions on Aerospace and Electronic Systems, 2018, 54(5): 2342-2355.</i>
                            <br>          
                          </p>
                          <a href="preprint/ZhangTAES2018preprint.pdf" target="_blank" textvalue="preprint/ZhangTAES2018preprint.pdf" class="pf-btn-view btn btn-primary">Preprint</a>
                          <a href="https://ieeexplore.ieee.org/document/8315479" target="_blank" textvalue="https://ieeexplore.ieee.org/document/8315479" class="pf-btn-view btn btn-primary">PDF link</a>
                          <a href="#pf-popup-ab-ZhangTAES2018" class="pf-btn-view btn btn-primary">Abstract</a>
                          <a href="#pf-popup-bib-ZhangTAES2018" class="pf-btn-view btn btn-primary">BibTeX</a>
                          <div class="pf-grid-item photography">
                            <div id="pf-popup-ab-ZhangTAES2018" class="pf-popup clearfix">
                                    <div class="pf-popup-info clear-mrg">
                                        <h2 class="text-upper">Abstract</h2>
                                        <p>This paper presents a novel vision-based method to solve the
                                          6-degree-of-freedom pose estimation problem of textureless space objects
                                          from a single monocular image. Our approach follows a coarse-to-fine procedure, utilizing only shape and contour information of the
                                          input image. To achieve invariance to initialization, we select a series
                                          of projection images that are similar to the input image and establish
                                          many-to-one 2D–3D correspondences by contour feature matching.
                                          Intensive attention is focused on outlier rejection and we introduce
                                          an innovative strategy to fully utilize geometric matching information
                                          to guide pose calculation. Experiments based on simulated images
                                          are carried out, and the results manifest that pose estimation error
                                          of our approach is about 1% even in situations with heavy outlier
                                          correspondences.                                            
                                        </p>
                                    </div><!-- .pf-popup-info -->
                            </div><!-- .pf-popup -->
                          </div><!-- .pf-grid-item -->
                          <div class="pf-grid-item photography">
                              <div id="pf-popup-bib-ZhangTAES2018" class="pf-popup clearfix">
                                      <div class="pf-popup-info clear-mrg">
                                          <h2 class="text-upper">BibTeX</h2>
                                          <p>
                                              @Article{ZhangTAES2018,<br> 
                                                author={X. {Zhang} and Z. {Jiang} and H. {Zhang} and Q. {Wei}},<br>
                                                journal={IEEE Transactions on Aerospace and Electronic Systems},<br>
                                                title={Vision-Based Pose Estimation for Textureless Space Objects by Contour Points Matching},<br>
                                                year={2018},<br>
                                                volume={54},<br>
                                                number={5},<br>
                                                pages={2342-2355},<br>
                                                doi={10.1109/TAES.2018.2815879},<br>
                                                ISSN={0018-9251},<br>
                                                month={Oct}<br>
                                                }
                                          </p>
                                      </div><!-- .pf-popup-info -->
                              </div><!-- .pf-popup -->
                            </div><!-- .pf-grid-item -->
                        </td>
                </tr> <!-- Paper End Here -->

            </tbody></table>

            <h3>Full List</h3>
            <p>
            <a href="https://orcid.org/0000-0003-1981-8307" target="_blank" textvalue="https://orcid.org/0000-0003-1981-8307">ORCID</a><br> 
            <a href="https://publons.com/researcher/1410897/haopeng-zhang/" target="_blank" textvalue="https://publons.com/researcher/1410897/haopeng-zhang/">publons</a><br> 
            <a href="https://scholar.google.com/citations?user=WTgprVgAAAAJ" target="_blank" textvalue="https://scholar.google.com/citations?user=WTgprVgAAAAJ">Google Scholar</a><br> 
            <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Haopeng" target="_blank" textvalue="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Haopeng">dblp
            </a><br>                        
            </p>

        </section><!-- .section -->


    </div><!-- .padd-box -->

<!-- END: PAGE CONTENT -->
						
                </div><!-- .ace-paper-cont -->
            </main><!-- .ace-paper -->
        </div><!-- .ace-paper-stock -->

        </div><!-- .ace-container -->
    </div><!-- #ace-content -->

    <footer id="ace-footer" class="ace-container-shift">
        <div class="ace-container">
            <div class="ace-footer-cont clear-mrg">
                <p class="text-center">Copyright &copy; 2019. All rights reserved.</p>
                <p class="text-center">Last Updated November 17, 2019.</p>	
            </div>
        </div><!-- .ace-container -->
    </footer><!-- #ace-footer -->

    <!-- Triangle Shapes -->
    <svg id="ace-bg-shape-1" class="hidden-sm hidden-xs" height="519" width="758">
        <polygon points="0,455,693,352,173,0,92,0,0,71" style="fill:#d2d2d2;stroke:purple;stroke-width:0; opacity: 0.5">
    </svg>

    <svg id="ace-bg-shape-2" class="hidden-sm hidden-xs" height="536" width="633">
        <polygon points="0,0,633,0,633,536" style="fill:#c0e3e7;stroke:purple;stroke-width:0" />
    </svg>
</div><!-- .ace-wrapper -->

<!-- Scripts -->
<script type="text/javascript" src="js/vendor/jquery-1.12.4.min.js"></script>


<!---<script type="text/javascript" src="http://ditu.google.cn/maps/api/js?key=AIzaSyDiwY_5J2Bkv2UgSeJa4NOKl6WUezSS9XA"></script>--->
<script type="text/javascript" src="js/plugins/highlight/highlight.pack.js"></script>
<script type="text/javascript" src="js/plugins/jquery.mCustomScrollbar.min.js"></script>
<script type="text/javascript" src="js/plugins/isotope.pkgd.min.js"></script>
<script type="text/javascript" src="js/plugins/progressbar.min.js"></script>
<script type="text/javascript" src="js/plugins/slick.min.js"></script>

<script type="text/javascript" src="js/options.js"></script>
<script type="text/javascript" src="js/main.js"></script>
</body>
</html>

